{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array, ImageDataGenerator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "sns.set_style('white')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST IMAGE DETAILS: \n",
      "Class:  <class 'PIL.JpegImagePlugin.JpegImageFile'> \n",
      "File Format:  JPEG \n",
      "Image type:  RGB \n",
      "Image Size: (640, 480)\n",
      "\n",
      "*************** Gray-scaling Images ***************\n",
      "\n",
      "*************** Converting Images to Arrays ***************\n"
     ]
    }
   ],
   "source": [
    "my_dir = os.getcwd()  # grabs current work dir\n",
    "all_file_paths = os.listdir()  # grabs all items in current work dir\n",
    "\n",
    "data_path = [data for data in all_file_paths if 'data' in data.lower()]  # searches for data folder and retrieves\n",
    "\n",
    "picture_folder_path = os.path.join(my_dir, data_path[0])  # joins data folder and current work dir to get picture path\n",
    "all_pictures = os.listdir(picture_folder_path)  # list of pictures names in folder\n",
    "\n",
    "picture_complete_path = [os.path.join(picture_folder_path, pic) for pic in\n",
    "                         all_pictures]  # complete path to each picture\n",
    "\n",
    "img_1 = load_img(picture_complete_path[0])\n",
    "print(\"FIRST IMAGE DETAILS:\", \"\\nClass: \", type(img_1), \"\\nFile Format: \", img_1.format,\n",
    "      \"\\nImage type: \", img_1.mode, '\\nImage Size:', img_1.size)\n",
    "print()\n",
    "\n",
    "grayscale_message = '*' * 15 + ' Gray-scaling Images ' + '*' * 15\n",
    "print(grayscale_message)\n",
    "grey_scale_photos = [load_img(photo, color_mode='grayscale') for photo in picture_complete_path]\n",
    "# convert all images to grayscale\n",
    "\n",
    "print()\n",
    "\n",
    "array_message = '*' * 15 + ' Converting Images to Arrays ' + '*' * 15\n",
    "print(array_message)\n",
    "array_photo_list = [img_to_array(photo) for photo in grey_scale_photos]\n",
    "# convert all grayscale images to arrays for ML\n",
    "X_train_all = np.asarray(array_photo_list)\n",
    "# X_train_all = X_train_all[:, :, :, 0]  # removes empty 4th dimension created from np.asarray()\n",
    "X_train_all[:, :,:,0] /= 255.0  # normalizes images\n",
    "\n",
    "del grey_scale_photos, array_photo_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# read in CSV as pandas DF\n",
    "df_y = pd.read_csv('TrainAnnotations.csv')\n",
    "\n",
    "# strip annotation column and convert to numpy\n",
    "Y_train_all = df_y['annotation'].to_numpy()\n",
    "\n",
    "# index position describes numeric values\n",
    "y_labels = ['No Wilting', 'Leaflets folding inward at secondary pulvinus, no turgor loss in leaflets or petioles',\n",
    "            'Slight leaflet or petiole turgor loss in upper canopy', 'Moderate turgor loss in upper canopy',\n",
    "            'Severe turgor loss throughout canopy']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>  Histogram of Underlying Y Label Distribution </center>\n",
    "\n",
    "<center> Highly unbalanced </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([array([640, 480]), 1], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y.describe()\n",
    "sns.distplot(Y_train_all, kde=False, bins=4)\n",
    "plt.show()\n",
    "np.array((np.asarray(img_1.size),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "# train_generator = train_datagen.flow_from_directory()\n",
    "\n",
    "model = keras.models.Sequential([keras.layers.Conv2D(filters=64, kernel_size=7, input_shape=(640, 480, 1)),\n",
    "                                 keras.layers.MaxPooling2D(pool_size=2),\n",
    "                                 keras.layers.Conv2D(filters=128, kernel_size=5),\n",
    "#                                  keras.layers.Conv2D(filters=128, kernel_size=3),\n",
    "                                 keras.layers.MaxPooling2D(pool_size=2),\n",
    "                                 keras.layers.Conv2D(filters=256, kernel_size=3),\n",
    "#                                  keras.layers.Conv2D(filters=256, kernel_size=3),\n",
    "                                 keras.layers.MaxPooling2D(pool_size=2),\n",
    "                                 keras.layers.Flatten(),\n",
    "                                 keras.layers.Dense(units=128, activation='relu'),\n",
    "                                 keras.layers.Dropout(0.5),\n",
    "                                 keras.layers.Dense(units=64, activation='relu'),\n",
    "#                                  keras.layers.Dropout(0.5),\n",
    "#                                  keras.layers.Dense(units=32, activation='relu'),\n",
    "#                                  keras.layers.Dropout(0.5),\n",
    "#                                  keras.layers.Dense(units=16, activation='relu'),\n",
    "                                 keras.layers.Dropout(0.5),\n",
    "                                 keras.layers.Dense(units=5, activation='softmax')\n",
    "                                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 634, 474, 64)      3200      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 317, 237, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 313, 233, 128)     204928    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 156, 116, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 154, 114, 256)     295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 77, 57, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1123584)           0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               143818880 \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 325       \n",
      "=================================================================\n",
      "Total params: 144,330,757\n",
      "Trainable params: 144,330,757\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected conv2d_input to have shape (640, 480, 1) but got array with shape (480, 640, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0e9e935d0c1b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train_all\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    233\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    236\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    237\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data_adapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_training_inputs\u001b[1;34m(model, x, y, batch_size, epochs, sample_weights, class_weights, steps_per_epoch, validation_split, validation_data, validation_steps, shuffle, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    550\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[0mcheck_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m         steps=steps_per_epoch)\n\u001b[0m\u001b[0;32m    553\u001b[0m     (x, y, sample_weights,\n\u001b[0;32m    554\u001b[0m      \u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2381\u001b[0m         \u001b[0mis_dataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2382\u001b[0m         \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2383\u001b[1;33m         batch_size=batch_size)\n\u001b[0m\u001b[0;32m   2384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2385\u001b[0m   def _standardize_tensors(self, x, y, sample_weight, run_eagerly, dict_inputs,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_tensors\u001b[1;34m(self, x, y, sample_weight, run_eagerly, dict_inputs, is_dataset, class_weight, batch_size)\u001b[0m\n\u001b[0;32m   2408\u001b[0m           \u001b[0mfeed_input_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2409\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2410\u001b[1;33m           exception_prefix='input')\n\u001b[0m\u001b[0;32m   2411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2412\u001b[0m     \u001b[1;31m# Get typespecs for the input data and sanitize it if necessary.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    580\u001b[0m                              \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m                              \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' but got array with shape '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 582\u001b[1;33m                              str(data_shape))\n\u001b[0m\u001b[0;32m    583\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking input: expected conv2d_input to have shape (640, 480, 1) but got array with shape (480, 640, 1)"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_all, Y_train_all, epochs=50, validation_split=0.2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
